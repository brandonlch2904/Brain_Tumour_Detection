{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNerpHhECGF/CK8MyE+fgQH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Setting Up Environment\n","\n","\n"],"metadata":{"id":"Lvl46cWpA2Cj"}},{"cell_type":"markdown","source":["## a. Installing Dependencies\n"],"metadata":{"id":"s1kVZmEO_lPC"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJA19BaG_b4w","executionInfo":{"status":"ok","timestamp":1749487731633,"user_tz":-480,"elapsed":148830,"user":{"displayName":"FYP","userId":"06650288711143266396"}},"outputId":"0b907b5b-9a7b-419b-91e7-8c105a6ac190"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting roboflow\n","  Downloading roboflow-1.1.66-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n","Collecting idna==3.7 (from roboflow)\n","  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n","Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n","  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n","Collecting pillow-heif>=0.18.0 (from roboflow)\n","  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n","Collecting python-dotenv (from roboflow)\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n","Collecting filetype (from roboflow)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n","Downloading roboflow-1.1.66-py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.11.0.86\n","    Uninstalling opencv-python-headless-4.11.0.86:\n","      Successfully uninstalled opencv-python-headless-4.11.0.86\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.66\n","Collecting optuna\n","  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n","Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, alembic, optuna\n","Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"]}],"source":["# PyTorch & Torchvision\n","!pip install --quiet torch torchvision torchaudio\n","\n","# Roboflow for dataset\n","!pip install roboflow\n","\n","# Optuna for Hyperparameter Optimization\n","!pip install optuna"]},{"cell_type":"markdown","source":["## b. Importing Libraries\n"],"metadata":{"id":"6l6McncgAAuB"}},{"cell_type":"code","source":["from roboflow import Roboflow\n","import os\n","import torch\n","import torch.optim as optim\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import torchvision.transforms as T\n","from torchvision import models\n","import matplotlib.pyplot as plt\n","import cv2\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    precision_recall_curve,\n","    classification_report,\n","    roc_curve,\n","    auc\n",")\n","from sklearn.preprocessing import label_binarize\n","import optuna\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from time import perf_counter"],"metadata":{"id":"95yRcOjlAQf1","executionInfo":{"status":"ok","timestamp":1749487762312,"user_tz":-480,"elapsed":30676,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## c. Download Dataset and Mount Drive"],"metadata":{"id":"w0iR9yvWAJ1R"}},{"cell_type":"code","source":["# Download Dataset\n","rf = Roboflow(api_key=\"SOWkZCal2FAKPG56WSnb\")\n","project = rf.workspace(\"work-tqclg\").project(\"tumor-cjxoh\")\n","version = project.version(1)\n","dataset = version.download(\"multiclass\", location=\"data\")\n","\n","# Mounting Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_7hkPuQkAEHV","executionInfo":{"status":"ok","timestamp":1749487811408,"user_tz":-480,"elapsed":49098,"user":{"displayName":"FYP","userId":"06650288711143266396"}},"outputId":"97b05074-f34b-4f3f-d3d6-5b7ebc6a0b7f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in data to multiclass:: 100%|██████████| 153390/153390 [00:05<00:00, 27527.47it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to data in multiclass:: 100%|██████████| 18356/18356 [00:06<00:00, 2963.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# 2. Model Training"],"metadata":{"id":"5-CZMFpSBCOb"}},{"cell_type":"markdown","source":["## a. Global Training Configuration"],"metadata":{"id":"zbRUiNMAD5NX"}},{"cell_type":"code","source":["DATA_DIR      = 'data'\n","TRAIN_IMG_DIR = os.path.join(DATA_DIR, 'train')\n","VALID_IMG_DIR = os.path.join(DATA_DIR, 'valid')\n","TEST_IMG_DIR  = os.path.join(DATA_DIR, 'test')\n","TRAIN_CSV     = os.path.join(TRAIN_IMG_DIR, '_classes.csv')\n","VALID_CSV     = os.path.join(VALID_IMG_DIR, '_classes.csv')\n","TEST_CSV      = os.path.join(TEST_IMG_DIR, '_classes.csv')\n","\n","BATCH_SIZE    = 32\n","MAX_EPOCHS    = 30\n","PATIENCE      = 5\n","\n","LR            = 1e-4\n","WEIGHT_DECAY  = 1e-3\n","STEP_SIZE     = 5\n","GAMMA         = 0.1\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","base_dir = \"/content/drive/MyDrive/Brandon's FYP\"\n","os.makedirs(base_dir, exist_ok=True)\n","OPTIMIZED_HYPERPARAMETERS = \"/content/drive/MyDrive/Brandon's FYP/Hyperparameter Optimization.xlsx\""],"metadata":{"id":"19bbK3WYC11z","executionInfo":{"status":"ok","timestamp":1749488066874,"user_tz":-480,"elapsed":5,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## b. Model Loader"],"metadata":{"id":"XIF6BQHkEHO2"}},{"cell_type":"code","source":["def get_all_models(num_classes):\n","    models_list = []\n","\n","    # ResNet50\n","    resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n","    resnet.fc = torch.nn.Sequential(\n","        torch.nn.Dropout(0.5),\n","        torch.nn.Linear(resnet.fc.in_features, num_classes)\n","    )\n","    models_list.append((\"ResNet50\", resnet))\n","\n","    # EfficientNetB2\n","    effnet = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.IMAGENET1K_V1)\n","    effnet.classifier[1] = torch.nn.Linear(effnet.classifier[1].in_features, num_classes)\n","    models_list.append((\"EfficientNetB2\", effnet))\n","\n","    # MobileNetV3\n","    mobile = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n","    mobile.classifier[3] = torch.nn.Linear(mobile.classifier[3].in_features, num_classes)\n","    models_list.append((\"MobileNetV3\", mobile))\n","\n","    return models_list\n"],"metadata":{"id":"Apkgs2HbEEr5","executionInfo":{"status":"ok","timestamp":1749487811837,"user_tz":-480,"elapsed":169,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## c. Dataset Class"],"metadata":{"id":"jKd5yCWXGdUo"}},{"cell_type":"code","source":["# ----------------------------\n","# Custom Dataset Class from CSV\n","# ----------------------------\n","class CSVClassificationDataset(torch.utils.data.Dataset):\n","    def __init__(self, csv_file, img_dir, transform=None):\n","        self.img_dir   = img_dir\n","        self.transform = transform\n","\n","        # Load CSV and infer class columns\n","        df = pd.read_csv(csv_file)\n","        df.columns = df.columns.str.strip()  # Clean column names\n","        self.class_names = [c for c in df.columns if c.lower() != 'filename']\n","        self.num_classes = len(self.class_names)\n","\n","        self.samples = [\n","            (row['filename'], int(np.argmax(row[self.class_names].values.astype(int))))\n","            for _, row in df.iterrows()\n","        ]\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        fname, label = self.samples[idx]\n","        img_path = os.path.join(self.img_dir, fname)\n","        img = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, torch.tensor(label, dtype=torch.long)"],"metadata":{"id":"dzig--8nGhE9","executionInfo":{"status":"ok","timestamp":1749487811843,"user_tz":-480,"elapsed":3,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## d. Data Preprocessing (Transformer)"],"metadata":{"id":"PNpJtjvzEmzJ"}},{"cell_type":"code","source":["# ----------------------------\n","# Flexible Preprocessing Function\n","# mode = 0 → Baseline (resize + normalize only)\n","# mode = 1 → Enhanced (blur, jitter, flip, rotate, normalize)\n","# ----------------------------\n","class GaussianBlur:\n","    def __init__(self, kernel_size=3):\n","        self.kernel_size = kernel_size if kernel_size % 2 == 1 else kernel_size + 1\n","\n","    def __call__(self, img):\n","        img_np = np.array(img)\n","        blurred = cv2.GaussianBlur(img_np, (self.kernel_size, self.kernel_size), 0)\n","        return Image.fromarray(blurred)\n","\n","def get_transforms(mode=0):\n","    # ImageNet Normalization\n","    normalize = T.Normalize([0.485, 0.456, 0.406],\n","                            [0.229, 0.224, 0.225])\n","\n","    if mode == 0:\n","        # Baseline: Resize + Normalize\n","        train_transform = T.Compose([\n","            T.Resize((224, 224)),\n","            T.ToTensor(),\n","            normalize,\n","        ])\n","        val_transform = train_transform\n","\n","    else:\n","        # Stage 2: Enhanced with noise reduction, contrast/brightness jitter, and augmentation\n","        train_transform = T.Compose([\n","            T.Resize((224, 224)),      # (1) Resize first\n","            T.RandomHorizontalFlip(),  # (2) Flip\n","            T.RandomRotation(15),      # (3) Rotate ±15°\n","            T.ColorJitter(\n","                brightness=0.1,   # ±10%\n","                contrast=0.1,     # ±10%\n","                saturation=0.1,   # ±10%\n","                hue=0.05          # ±5%\n","            ),\n","            T.RandomApply(\n","                [GaussianBlur(kernel_size=3)],  # (5) Small blur on only some images\n","                p=0.3\n","            ),\n","            T.ToTensor(),\n","            normalize,\n","        ])\n","\n","        val_transform = T.Compose([\n","            T.Resize((224, 224)),\n","            T.ToTensor(),\n","            normalize,\n","        ])\n","\n","    return train_transform, val_transform\n"],"metadata":{"id":"kq7zDmk1EuHX","executionInfo":{"status":"ok","timestamp":1749487811868,"user_tz":-480,"elapsed":14,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## e. Main Training Function"],"metadata":{"id":"GVoogvb5Ec5X"}},{"cell_type":"code","source":["# ----------------------------\n","# Evaluation Function\n","# ----------------------------\n","def evaluate(model, loader, device):\n","    model.eval()\n","    correct = total = 0\n","    with torch.no_grad():\n","        for imgs, labels in loader:\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            preds = torch.argmax(model(imgs), dim=1)\n","            correct += (preds == labels).sum().item()\n","            total   += labels.size(0)\n","    return correct / total\n","\n","# ----------------------------\n","# Train Function\n","# ----------------------------\n","def train_model(model, model_name, train_loader, val_loader):\n","    # Define the model-specific directory under base_dir\n","    model_root_dir = os.path.join(base_dir, model_name)\n","\n","    # Auto-versioning inside that model folder\n","    version = 1\n","    save_dir = os.path.join(model_root_dir, f\"run_v{version}\")\n","    while os.path.exists(save_dir):\n","        version += 1\n","        save_dir = os.path.join(model_root_dir, f\"run_v{version}\")\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    print(f\"[{model_name}] Saving checkpoints & plots to: {save_dir}\")\n","\n","    # Training loop as before\n","    model = model.to(DEVICE)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=WEIGHT_DECAY)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","    best_acc = 0.0\n","    train_losses = []\n","    val_accuracies = []\n","    epochs_no_improve = 0\n","\n","    print(f\"\\n--- Starts Training: {model_name} ---\")\n","    for epoch in range(1, MAX_EPOCHS + 1):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for imgs, labels in train_loader:\n","            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n","            optimizer.zero_grad()\n","            loss = criterion(model(imgs), labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        scheduler.step()\n","\n","        avg_loss = running_loss / len(train_loader)\n","        train_losses.append(avg_loss)\n","\n","        acc = evaluate(model, val_loader, DEVICE)\n","        val_accuracies.append(acc)\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            epochs_no_improve = 0\n","            torch.save(model.state_dict(), os.path.join(save_dir, 'best.pth'))\n","            print(f\"[{model_name}] Epoch {epoch:02d}: ⬆ New best val_acc: {acc:.4f} | Train Loss: {avg_loss:.4f}\")\n","        else:\n","            epochs_no_improve += 1\n","            print(f\"[{model_name}] Epoch {epoch:02d}: — val_acc did not improve ({acc:.4f}); \"\n","                  f\"patience {epochs_no_improve}/{PATIENCE} | Train Loss: {avg_loss:.4f}\")\n","\n","        torch.save(model.state_dict(), os.path.join(save_dir, 'last.pth'))\n","\n","        if epochs_no_improve >= PATIENCE:\n","            print(f\"Early stopping triggered for {model_name}\")\n","            break\n","\n","    # Save plots\n","    epochs = range(1, len(train_losses) + 1)\n","\n","    plt.figure()\n","    plt.plot(epochs, train_losses, 'o-', label=\"Train Loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(f\"{model_name} Loss Curve\")\n","    plt.legend(); plt.grid()\n","    plt.savefig(os.path.join(save_dir, \"loss_curve.png\")); plt.close()\n","\n","    plt.figure()\n","    plt.plot(epochs, val_accuracies, 's-', label=\"Val Accuracy\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.title(f\"{model_name} Accuracy Curve\")\n","    plt.legend(); plt.grid()\n","    plt.savefig(os.path.join(save_dir, \"val_accuracy_curve.png\")); plt.close()"],"metadata":{"id":"Fq_rqw9sEfeh","executionInfo":{"status":"ok","timestamp":1749487811905,"user_tz":-480,"elapsed":35,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## f. Stage Training Functions\n","\n","### What I Did (Stage 2 Pipeline)\n","* Gaussian Blur (kernel 5×5) applied to every training and validation image.\n","\n","* Resize to 224×224 immediately after blurring.\n","\n","* ColorJitter (brightness ±20 %, contrast ±20 %, saturation ±20 %, hue ±10 %).\n","\n","* Early Stopping (patience = 5) and checkpoints for “best” and “last” models.\n","\n","### What I’ll Change (Based on Suggestions)\n","* Remove Gaussian Blur from Validation\n","\n","* Keep val_transform = Resize(224,224) → ToTensor → Normalize.\n","\n","  * This ensures validation images remain sharp and realistic.\n","\n","* Apply a Milder Blur on Train (Only Occasionally)\n","\n","* Replace GaussianBlur(kernel_size=5) with RandomApply([GaussianBlur(kernel_size=3)], p=0.3).\n","\n","* Blur only about 30 % of training images, and use a 3×3 kernel to preserve tumor edges.\n","\n","* Reorder & Soften Transforms\n","\n","* New order:\n","\n","  * Resize(224,224)\n","\n","  * RandomHorizontalFlip\n","\n","  * RandomRotation (±15°)\n","\n","  * ColorJitter (brightness ±10 %, contrast ±10 %, saturation ±10 %, hue ±5 %)\n","\n","  * Occasional GaussianBlur(3×3)\n","\n","  * ToTensor → Normalize\n","\n","  * Smaller jitter reduces the chance of washing out faint tumor‐vs‐tissue contrast."],"metadata":{"id":"sKTUzrx3JFbB"}},{"cell_type":"code","source":["# ----------------------------\n","# Stage 1: Baseline Training\n","# ----------------------------\n","def stage_1_baseline_training():\n","    print(\"\\n==== Stage 1: Baseline Training ====\\n\")\n","\n","    # Get transforms for Stage 1 (mode=0)\n","    train_transform, val_transform = get_transforms(mode=0)\n","\n","    # Create datasets and data loaders\n","    train_ds = CSVClassificationDataset(TRAIN_CSV, TRAIN_IMG_DIR, transform=train_transform)\n","    val_ds   = CSVClassificationDataset(VALID_CSV, VALID_IMG_DIR, transform=val_transform)\n","\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n","    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","    # Get all models\n","    models_list = get_all_models(num_classes=train_ds.num_classes)\n","\n","    # Loop through models and train each one\n","    for model_name, model in models_list:\n","        print(f\"\\n--- Training {model_name} ---\")\n","        train_model(model, model_name, train_loader, val_loader)\n","\n","# ----------------------------\n","# Stage 2: Preprocessing + Augmentation Training\n","# ----------------------------\n","def stage_2_preprocessing_training():\n","    print(\"\\n==== Stage 2: Training with Preprocessing & Augmentation ====\\n\")\n","\n","    # Get enhanced transforms (mode=1)\n","    train_transform, val_transform = get_transforms(mode=1)\n","\n","    # Prepare datasets and loaders\n","    train_ds = CSVClassificationDataset(TRAIN_CSV, TRAIN_IMG_DIR, transform=train_transform)\n","    val_ds   = CSVClassificationDataset(VALID_CSV, VALID_IMG_DIR, transform=val_transform)\n","\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n","    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","    # Load models\n","    models_list = get_all_models(num_classes=train_ds.num_classes)\n","\n","    # Train each model\n","    for model_name, model in models_list:\n","        print(f\"\\n--- Stage 2 Training: {model_name} ---\")\n","        train_model(model, model_name, train_loader, val_loader)\n","\n","# ----------------------------\n","# Stage 3: Hyperparameter Optimization with Optuna (n_trials per model)\n","# ----------------------------\n","def stage_3_hyperparameter_optimization(n_trials=20):\n","    print(f\"\\n==== Stage 3: Hyperparameter Search ({n_trials} trials per model) ====\\n\")\n","\n","    def objective_for_model(fixed_model_name, trial):\n","        # 1. Sample hyperparameters (excluding model_name, which is fixed)\n","        lr           = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n","        batch_size   = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n","        weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n","        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n","        optimizer    = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"adam\"])\n","        momentum     = trial.suggest_float(\"momentum\", 0.5, 0.99)\n","        scheduler    = trial.suggest_categorical(\"scheduler\", [\"none\", \"steplr\", \"cosineannealing\"])\n","\n","        # Print trial info\n","        print(f\"\\n--- {fixed_model_name} Trial {trial.number + 1}/{n_trials} ---\")\n","        print(f\"LR: {lr:.2e} | Batch: {batch_size} | WD: {weight_decay:.2e} | \"\n","              f\"Dropout: {dropout_rate:.2f} | Opt: {optimizer} | Momentum: {momentum:.2f} | Scheduler: {scheduler}\")\n","\n","        # 2. Build DataLoaders with stage 3 transforms\n","        train_transform, val_transform = get_transforms(mode=1)\n","        train_ds = CSVClassificationDataset(TRAIN_CSV, TRAIN_IMG_DIR, transform=train_transform)\n","        val_ds   = CSVClassificationDataset(VALID_CSV, VALID_IMG_DIR, transform=val_transform)\n","\n","        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2)\n","        val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","        # 3. Instantiate fixed_model_name from get_all_models and replace dropout\n","        models_list = get_all_models(train_ds.num_classes)\n","        model = None\n","        for name, m in models_list:\n","            if name == fixed_model_name:\n","                if fixed_model_name == \"ResNet50\":\n","                    m.fc[0] = torch.nn.Dropout(dropout_rate)\n","                elif fixed_model_name == \"EfficientNetB2\":\n","                    m.classifier.insert(0, torch.nn.Dropout(dropout_rate))\n","                elif fixed_model_name == \"MobileNetV3\":\n","                    m.classifier.insert(0, torch.nn.Dropout(dropout_rate))\n","                model = m.to(DEVICE)\n","                break\n","\n","        # 4. Choose optimizer\n","        if optimizer == \"sgd\":\n","            optim_inst = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","        else:\n","            optim_inst = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","        # 5. Choose scheduler\n","        if scheduler == \"steplr\":\n","            scheduler_inst = optim.lr_scheduler.StepLR(optim_inst, step_size=5, gamma=0.1)\n","        elif scheduler == \"cosineannealing\":\n","            scheduler_inst = optim.lr_scheduler.CosineAnnealingLR(optim_inst, T_max=10)\n","        else:\n","            scheduler_inst = None\n","\n","        criterion = nn.CrossEntropyLoss()\n","        best_val_acc = 0.0\n","\n","        # 6. Train for fixed 10 epochs per trial\n","        for epoch in range(10):\n","            model.train()\n","            for imgs, labels in train_loader:\n","                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n","                optim_inst.zero_grad()\n","                outputs = model(imgs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optim_inst.step()\n","\n","            if scheduler_inst:\n","                scheduler_inst.step()\n","\n","            # 7. Validation\n","            model.eval()\n","            correct = 0\n","            total = 0\n","            with torch.no_grad():\n","                for imgs, labels in val_loader:\n","                    imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n","                    outputs = model(imgs)\n","                    preds = outputs.argmax(dim=1)\n","                    correct += (preds == labels).sum().item()\n","                    total += labels.size(0)\n","            val_acc = correct / total\n","\n","            # Print epoch progress\n","            print(f\"{fixed_model_name} Trial {trial.number + 1}, Epoch {epoch + 1}/10, Val Acc: {val_acc:.4f}\")\n","\n","            trial.report(val_acc, epoch)\n","            if trial.should_prune():\n","                print(f\"{fixed_model_name} Trial {trial.number + 1} pruned at epoch {epoch + 1}\")\n","                raise optuna.TrialPruned()\n","\n","            best_val_acc = max(best_val_acc, val_acc)\n","\n","        return best_val_acc\n","\n","    # 8. Loop over each model, run n_trials per model\n","    all_best_configs = []\n","    for fixed_model_name in [\"ResNet50\", \"EfficientNetB2\", \"MobileNetV3\"]:\n","        print(f\"\\n===== Optimizing {fixed_model_name} =====\")\n","        study = optuna.create_study(direction=\"maximize\")\n","        func = lambda trial: objective_for_model(fixed_model_name, trial)\n","        study.optimize(func, n_trials=n_trials)\n","\n","        # 9. Extract top 3 configs for this model\n","        df_trials = study.trials_dataframe()\n","        # Tag every trial with fixed_model_name, since this study only tuned that model\n","        df_trials[\"model\"] = fixed_model_name\n","        # Sort all trials by “value” and take the top 3\n","        df_m_sorted = df_trials.sort_values(\"value\", ascending=False).head(3)\n","\n","        for rank, row in enumerate(df_m_sorted.itertuples(), start=1):\n","            all_best_configs.append({\n","                \"model\":        row.model,  # same as fixed_model_name\n","                \"rank\":         rank,\n","                \"learning_rate\": row.params_learning_rate,\n","                \"batch_size\":    row.params_batch_size,\n","                \"weight_decay\":  row.params_weight_decay,\n","                \"dropout_rate\":  row.params_dropout_rate,\n","                \"optimizer\":     row.params_optimizer,\n","                \"momentum\":      row.params_momentum,\n","                \"scheduler\":     row.params_scheduler,\n","                \"val_accuracy\":  row.value\n","            })\n","\n","    # 10. Save all results to CSV\n","    df_best = pd.DataFrame(all_best_configs)\n","    df_best = df_best[[\n","        \"model\", \"rank\", \"learning_rate\", \"batch_size\", \"weight_decay\",\n","        \"dropout_rate\", \"optimizer\", \"momentum\", \"scheduler\", \"val_accuracy\"\n","    ]]\n","    summary_path = \"/content/drive/MyDrive/Brandon's FYP/hparam_stage3_summary.csv\"\n","    df_best.to_csv(summary_path, index=False)\n","\n","    print(\"\\n=== Top 3 Configurations per Model ===\")\n","    print(df_best)\n","    print(f\"\\nSummary saved to: {summary_path}\")"],"metadata":{"id":"OTxNwtV8JET9","executionInfo":{"status":"ok","timestamp":1749487811943,"user_tz":-480,"elapsed":37,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Main Training Launcher"],"metadata":{"id":"yid6Ue3DIDZv"}},{"cell_type":"code","source":["# ----------------------------\n","# Main Launcher\n","# ----------------------------\n","if __name__ == '__main__':\n","    # stage_1_baseline_training()\n","    # stage_2_preprocessing_training()\n","    stage_3_hyperparameter_optimization(n_trials=20) # 9 hours, 49 minutes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w0Iu9tpCICgj","outputId":"0910ebeb-3d77-4fe3-934f-379b110478d9","executionInfo":{"status":"ok","timestamp":1749271322728,"user_tz":-480,"elapsed":35342766,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 18:53:00,682] A new study created in memory with name: no-name-36ac87af-7f93-4066-9cdb-59739e073a86\n"]},{"output_type":"stream","name":"stdout","text":["\n","==== Stage 3: Hyperparameter Search (20 trials per model) ====\n","\n","\n","===== Optimizing ResNet50 =====\n","\n","--- ResNet50 Trial 1/20 ---\n","LR: 3.13e-03 | Batch: 16 | WD: 4.86e-04 | Dropout: 0.32 | Opt: sgd | Momentum: 0.88 | Scheduler: cosineannealing\n","ResNet50 Trial 1, Epoch 1/10, Val Acc: 0.9530\n","ResNet50 Trial 1, Epoch 2/10, Val Acc: 0.9775\n","ResNet50 Trial 1, Epoch 3/10, Val Acc: 0.9826\n","ResNet50 Trial 1, Epoch 4/10, Val Acc: 0.9777\n","ResNet50 Trial 1, Epoch 5/10, Val Acc: 0.9867\n","ResNet50 Trial 1, Epoch 6/10, Val Acc: 0.9902\n","ResNet50 Trial 1, Epoch 7/10, Val Acc: 0.9883\n","ResNet50 Trial 1, Epoch 8/10, Val Acc: 0.9889\n","ResNet50 Trial 1, Epoch 9/10, Val Acc: 0.9894\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 19:17:19,287] Trial 0 finished with value: 0.9902253597610643 and parameters: {'learning_rate': 0.0031315785742164507, 'batch_size': 16, 'weight_decay': 0.0004864230118092582, 'dropout_rate': 0.3174454796810835, 'optimizer': 'sgd', 'momentum': 0.8762104225545309, 'scheduler': 'cosineannealing'}. Best is trial 0 with value: 0.9902253597610643.\n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 1, Epoch 10/10, Val Acc: 0.9889\n","\n","--- ResNet50 Trial 2/20 ---\n","LR: 7.69e-05 | Batch: 32 | WD: 4.13e-06 | Dropout: 0.48 | Opt: adam | Momentum: 0.75 | Scheduler: cosineannealing\n","ResNet50 Trial 2, Epoch 1/10, Val Acc: 0.9661\n","ResNet50 Trial 2, Epoch 2/10, Val Acc: 0.9772\n","ResNet50 Trial 2, Epoch 3/10, Val Acc: 0.9851\n","ResNet50 Trial 2, Epoch 4/10, Val Acc: 0.9883\n","ResNet50 Trial 2, Epoch 5/10, Val Acc: 0.9886\n","ResNet50 Trial 2, Epoch 6/10, Val Acc: 0.9867\n","ResNet50 Trial 2, Epoch 7/10, Val Acc: 0.9894\n","ResNet50 Trial 2, Epoch 8/10, Val Acc: 0.9905\n","ResNet50 Trial 2, Epoch 9/10, Val Acc: 0.9889\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 19:40:54,488] Trial 1 finished with value: 0.9904968775454792 and parameters: {'learning_rate': 7.690160125267548e-05, 'batch_size': 32, 'weight_decay': 4.130399837032214e-06, 'dropout_rate': 0.4840021733729226, 'optimizer': 'adam', 'momentum': 0.7521860761600443, 'scheduler': 'cosineannealing'}. Best is trial 1 with value: 0.9904968775454792.\n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 2, Epoch 10/10, Val Acc: 0.9900\n","\n","--- ResNet50 Trial 3/20 ---\n","LR: 2.13e-05 | Batch: 32 | WD: 6.63e-04 | Dropout: 0.13 | Opt: adam | Momentum: 0.68 | Scheduler: none\n","ResNet50 Trial 3, Epoch 1/10, Val Acc: 0.9511\n","ResNet50 Trial 3, Epoch 2/10, Val Acc: 0.9739\n","ResNet50 Trial 3, Epoch 3/10, Val Acc: 0.9720\n","ResNet50 Trial 3, Epoch 4/10, Val Acc: 0.9821\n","ResNet50 Trial 3, Epoch 5/10, Val Acc: 0.9815\n","ResNet50 Trial 3, Epoch 6/10, Val Acc: 0.9837\n","ResNet50 Trial 3, Epoch 7/10, Val Acc: 0.9848\n","ResNet50 Trial 3, Epoch 8/10, Val Acc: 0.9856\n","ResNet50 Trial 3, Epoch 9/10, Val Acc: 0.9837\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 20:04:29,132] Trial 2 finished with value: 0.9877816997013305 and parameters: {'learning_rate': 2.1262443576960974e-05, 'batch_size': 32, 'weight_decay': 0.0006625223901495683, 'dropout_rate': 0.12675138836040156, 'optimizer': 'adam', 'momentum': 0.6777539228814079, 'scheduler': 'none'}. Best is trial 1 with value: 0.9904968775454792.\n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 3, Epoch 10/10, Val Acc: 0.9878\n","\n","--- ResNet50 Trial 4/20 ---\n","LR: 1.92e-03 | Batch: 16 | WD: 4.63e-04 | Dropout: 0.15 | Opt: adam | Momentum: 0.70 | Scheduler: none\n","ResNet50 Trial 4, Epoch 1/10, Val Acc: 0.7049\n","ResNet50 Trial 4, Epoch 2/10, Val Acc: 0.5631\n","ResNet50 Trial 4, Epoch 3/10, Val Acc: 0.8414\n","ResNet50 Trial 4, Epoch 4/10, Val Acc: 0.7013\n","ResNet50 Trial 4, Epoch 5/10, Val Acc: 0.8618\n","ResNet50 Trial 4, Epoch 6/10, Val Acc: 0.8952\n","ResNet50 Trial 4, Epoch 7/10, Val Acc: 0.7706\n","ResNet50 Trial 4, Epoch 8/10, Val Acc: 0.9294\n","ResNet50 Trial 4, Epoch 9/10, Val Acc: 0.8689\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 20:28:25,881] Trial 3 finished with value: 0.9294053760521314 and parameters: {'learning_rate': 0.0019195361223879597, 'batch_size': 16, 'weight_decay': 0.00046335082821222733, 'dropout_rate': 0.15487139716496795, 'optimizer': 'adam', 'momentum': 0.7023370025355749, 'scheduler': 'none'}. Best is trial 1 with value: 0.9904968775454792.\n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 4, Epoch 10/10, Val Acc: 0.5585\n","\n","--- ResNet50 Trial 5/20 ---\n","LR: 9.71e-05 | Batch: 16 | WD: 7.01e-05 | Dropout: 0.40 | Opt: adam | Momentum: 0.53 | Scheduler: cosineannealing\n","ResNet50 Trial 5, Epoch 1/10, Val Acc: 0.9661\n","ResNet50 Trial 5, Epoch 2/10, Val Acc: 0.9829\n","ResNet50 Trial 5, Epoch 3/10, Val Acc: 0.9834\n","ResNet50 Trial 5, Epoch 4/10, Val Acc: 0.9902\n","ResNet50 Trial 5, Epoch 5/10, Val Acc: 0.9843\n","ResNet50 Trial 5, Epoch 6/10, Val Acc: 0.9919\n","ResNet50 Trial 5, Epoch 7/10, Val Acc: 0.9924\n","ResNet50 Trial 5, Epoch 8/10, Val Acc: 0.9940\n","ResNet50 Trial 5, Epoch 9/10, Val Acc: 0.9929\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 20:53:08,349] Trial 4 finished with value: 0.9940266087428726 and parameters: {'learning_rate': 9.71210831569226e-05, 'batch_size': 16, 'weight_decay': 7.011833315502553e-05, 'dropout_rate': 0.40369263986354786, 'optimizer': 'adam', 'momentum': 0.5345325857250904, 'scheduler': 'cosineannealing'}. Best is trial 4 with value: 0.9940266087428726.\n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 5, Epoch 10/10, Val Acc: 0.9924\n","\n","--- ResNet50 Trial 6/20 ---\n","LR: 7.36e-03 | Batch: 16 | WD: 1.52e-05 | Dropout: 0.29 | Opt: sgd | Momentum: 0.90 | Scheduler: steplr\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 20:55:39,675] Trial 5 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 6, Epoch 1/10, Val Acc: 0.9343\n","ResNet50 Trial 6 pruned at epoch 1\n","\n","--- ResNet50 Trial 7/20 ---\n","LR: 1.20e-05 | Batch: 64 | WD: 2.53e-06 | Dropout: 0.26 | Opt: sgd | Momentum: 0.78 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 20:58:05,334] Trial 6 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 7, Epoch 1/10, Val Acc: 0.2256\n","ResNet50 Trial 7 pruned at epoch 1\n","\n","--- ResNet50 Trial 8/20 ---\n","LR: 1.80e-03 | Batch: 16 | WD: 9.10e-06 | Dropout: 0.42 | Opt: adam | Momentum: 0.94 | Scheduler: steplr\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:00:39,726] Trial 7 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 8, Epoch 1/10, Val Acc: 0.8908\n","ResNet50 Trial 8 pruned at epoch 1\n","\n","--- ResNet50 Trial 9/20 ---\n","LR: 1.21e-05 | Batch: 64 | WD: 1.33e-06 | Dropout: 0.21 | Opt: sgd | Momentum: 0.54 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:03:05,609] Trial 8 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 9, Epoch 1/10, Val Acc: 0.2199\n","ResNet50 Trial 9 pruned at epoch 1\n","\n","--- ResNet50 Trial 10/20 ---\n","LR: 1.10e-05 | Batch: 32 | WD: 5.34e-05 | Dropout: 0.19 | Opt: sgd | Momentum: 0.68 | Scheduler: steplr\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:05:31,614] Trial 9 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 10, Epoch 1/10, Val Acc: 0.3310\n","ResNet50 Trial 10 pruned at epoch 1\n","\n","--- ResNet50 Trial 11/20 ---\n","LR: 2.43e-04 | Batch: 16 | WD: 6.62e-05 | Dropout: 0.38 | Opt: adam | Momentum: 0.51 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:08:06,111] Trial 10 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 11, Epoch 1/10, Val Acc: 0.9506\n","ResNet50 Trial 11 pruned at epoch 1\n","\n","--- ResNet50 Trial 12/20 ---\n","LR: 9.38e-05 | Batch: 32 | WD: 1.21e-04 | Dropout: 0.50 | Opt: adam | Momentum: 0.59 | Scheduler: cosineannealing\n","ResNet50 Trial 12, Epoch 1/10, Val Acc: 0.9685\n","ResNet50 Trial 12, Epoch 2/10, Val Acc: 0.9802\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:15:16,329] Trial 11 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 12, Epoch 3/10, Val Acc: 0.9750\n","ResNet50 Trial 12 pruned at epoch 3\n","\n","--- ResNet50 Trial 13/20 ---\n","LR: 9.13e-05 | Batch: 32 | WD: 8.67e-06 | Dropout: 0.50 | Opt: adam | Momentum: 0.78 | Scheduler: cosineannealing\n","ResNet50 Trial 13, Epoch 1/10, Val Acc: 0.9718\n","ResNet50 Trial 13, Epoch 2/10, Val Acc: 0.9815\n","ResNet50 Trial 13, Epoch 3/10, Val Acc: 0.9900\n","ResNet50 Trial 13, Epoch 4/10, Val Acc: 0.9802\n","ResNet50 Trial 13, Epoch 5/10, Val Acc: 0.9829\n","ResNet50 Trial 13, Epoch 6/10, Val Acc: 0.9872\n","ResNet50 Trial 13, Epoch 7/10, Val Acc: 0.9878\n","ResNet50 Trial 13, Epoch 8/10, Val Acc: 0.9894\n","ResNet50 Trial 13, Epoch 9/10, Val Acc: 0.9881\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:38:56,552] Trial 12 finished with value: 0.9899538419766495 and parameters: {'learning_rate': 9.12528072408054e-05, 'batch_size': 32, 'weight_decay': 8.666756605695208e-06, 'dropout_rate': 0.49887129899646926, 'optimizer': 'adam', 'momentum': 0.7785794898215443, 'scheduler': 'cosineannealing'}. Best is trial 4 with value: 0.9940266087428726.\n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 13, Epoch 10/10, Val Acc: 0.9883\n","\n","--- ResNet50 Trial 14/20 ---\n","LR: 5.83e-05 | Batch: 32 | WD: 3.48e-06 | Dropout: 0.04 | Opt: adam | Momentum: 0.61 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:41:25,754] Trial 13 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 14, Epoch 1/10, Val Acc: 0.9557\n","ResNet50 Trial 14 pruned at epoch 1\n","\n","--- ResNet50 Trial 15/20 ---\n","LR: 4.32e-04 | Batch: 64 | WD: 2.62e-05 | Dropout: 0.40 | Opt: adam | Momentum: 0.80 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:43:52,966] Trial 14 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 15, Epoch 1/10, Val Acc: 0.8604\n","ResNet50 Trial 15 pruned at epoch 1\n","\n","--- ResNet50 Trial 16/20 ---\n","LR: 4.18e-04 | Batch: 16 | WD: 1.78e-04 | Dropout: 0.45 | Opt: adam | Momentum: 0.83 | Scheduler: none\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:46:27,370] Trial 15 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 16, Epoch 1/10, Val Acc: 0.9411\n","ResNet50 Trial 16 pruned at epoch 1\n","\n","--- ResNet50 Trial 17/20 ---\n","LR: 4.35e-05 | Batch: 32 | WD: 4.26e-06 | Dropout: 0.34 | Opt: adam | Momentum: 0.61 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:48:52,845] Trial 16 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 17, Epoch 1/10, Val Acc: 0.9514\n","ResNet50 Trial 17 pruned at epoch 1\n","\n","--- ResNet50 Trial 18/20 ---\n","LR: 1.84e-04 | Batch: 16 | WD: 2.70e-05 | Dropout: 0.45 | Opt: adam | Momentum: 0.72 | Scheduler: cosineannealing\n","ResNet50 Trial 18, Epoch 1/10, Val Acc: 0.9595\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:53:53,121] Trial 17 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 18, Epoch 2/10, Val Acc: 0.9715\n","ResNet50 Trial 18 pruned at epoch 2\n","\n","--- ResNet50 Trial 19/20 ---\n","LR: 7.44e-04 | Batch: 32 | WD: 1.78e-04 | Dropout: 0.35 | Opt: adam | Momentum: 0.65 | Scheduler: none\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 21:56:20,944] Trial 18 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 19, Epoch 1/10, Val Acc: 0.9362\n","ResNet50 Trial 19 pruned at epoch 1\n","\n","--- ResNet50 Trial 20/20 ---\n","LR: 1.38e-04 | Batch: 64 | WD: 1.04e-06 | Dropout: 0.43 | Opt: adam | Momentum: 0.98 | Scheduler: steplr\n","ResNet50 Trial 20, Epoch 1/10, Val Acc: 0.9620\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 22:01:06,634] Trial 19 pruned. \n","[I 2025-06-06 22:01:06,645] A new study created in memory with name: no-name-aa44fac5-67e9-4009-9030-29a264415471\n"]},{"output_type":"stream","name":"stdout","text":["ResNet50 Trial 20, Epoch 2/10, Val Acc: 0.9745\n","ResNet50 Trial 20 pruned at epoch 2\n","\n","===== Optimizing EfficientNetB2 =====\n","\n","--- EfficientNetB2 Trial 1/20 ---\n","LR: 1.21e-03 | Batch: 64 | WD: 2.03e-05 | Dropout: 0.08 | Opt: adam | Momentum: 0.73 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 1, Epoch 1/10, Val Acc: 0.9644\n","EfficientNetB2 Trial 1, Epoch 2/10, Val Acc: 0.9843\n","EfficientNetB2 Trial 1, Epoch 3/10, Val Acc: 0.9829\n","EfficientNetB2 Trial 1, Epoch 4/10, Val Acc: 0.9875\n","EfficientNetB2 Trial 1, Epoch 5/10, Val Acc: 0.9883\n","EfficientNetB2 Trial 1, Epoch 6/10, Val Acc: 0.9919\n","EfficientNetB2 Trial 1, Epoch 7/10, Val Acc: 0.9913\n","EfficientNetB2 Trial 1, Epoch 8/10, Val Acc: 0.9927\n","EfficientNetB2 Trial 1, Epoch 9/10, Val Acc: 0.9940\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 22:19:08,001] Trial 0 finished with value: 0.9945696443117024 and parameters: {'learning_rate': 0.0012112745479495397, 'batch_size': 64, 'weight_decay': 2.0284419798817362e-05, 'dropout_rate': 0.07839650780739826, 'optimizer': 'adam', 'momentum': 0.7329424514948062, 'scheduler': 'cosineannealing'}. Best is trial 0 with value: 0.9945696443117024.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 1, Epoch 10/10, Val Acc: 0.9946\n","\n","--- EfficientNetB2 Trial 2/20 ---\n","LR: 1.44e-04 | Batch: 16 | WD: 7.22e-04 | Dropout: 0.03 | Opt: sgd | Momentum: 0.93 | Scheduler: none\n","EfficientNetB2 Trial 2, Epoch 1/10, Val Acc: 0.8756\n","EfficientNetB2 Trial 2, Epoch 2/10, Val Acc: 0.9196\n","EfficientNetB2 Trial 2, Epoch 3/10, Val Acc: 0.9370\n","EfficientNetB2 Trial 2, Epoch 4/10, Val Acc: 0.9408\n","EfficientNetB2 Trial 2, Epoch 5/10, Val Acc: 0.9484\n","EfficientNetB2 Trial 2, Epoch 6/10, Val Acc: 0.9612\n","EfficientNetB2 Trial 2, Epoch 7/10, Val Acc: 0.9642\n","EfficientNetB2 Trial 2, Epoch 8/10, Val Acc: 0.9688\n","EfficientNetB2 Trial 2, Epoch 9/10, Val Acc: 0.9707\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 22:39:34,635] Trial 1 finished with value: 0.9717621504208526 and parameters: {'learning_rate': 0.00014441234764570306, 'batch_size': 16, 'weight_decay': 0.0007221244635879038, 'dropout_rate': 0.03132714392785285, 'optimizer': 'sgd', 'momentum': 0.933183904961127, 'scheduler': 'none'}. Best is trial 0 with value: 0.9945696443117024.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 2, Epoch 10/10, Val Acc: 0.9718\n","\n","--- EfficientNetB2 Trial 3/20 ---\n","LR: 1.04e-03 | Batch: 64 | WD: 8.45e-05 | Dropout: 0.05 | Opt: sgd | Momentum: 0.61 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 3, Epoch 1/10, Val Acc: 0.8042\n","EfficientNetB2 Trial 3, Epoch 2/10, Val Acc: 0.8618\n","EfficientNetB2 Trial 3, Epoch 3/10, Val Acc: 0.8773\n","EfficientNetB2 Trial 3, Epoch 4/10, Val Acc: 0.8906\n","EfficientNetB2 Trial 3, Epoch 5/10, Val Acc: 0.8993\n","EfficientNetB2 Trial 3, Epoch 6/10, Val Acc: 0.9080\n","EfficientNetB2 Trial 3, Epoch 7/10, Val Acc: 0.9082\n","EfficientNetB2 Trial 3, Epoch 8/10, Val Acc: 0.9058\n","EfficientNetB2 Trial 3, Epoch 9/10, Val Acc: 0.9147\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 22:57:37,727] Trial 2 finished with value: 0.914743415693728 and parameters: {'learning_rate': 0.001038371256161337, 'batch_size': 64, 'weight_decay': 8.445111304456727e-05, 'dropout_rate': 0.052875513596335355, 'optimizer': 'sgd', 'momentum': 0.6102083326276432, 'scheduler': 'cosineannealing'}. Best is trial 0 with value: 0.9945696443117024.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 3, Epoch 10/10, Val Acc: 0.9112\n","\n","--- EfficientNetB2 Trial 4/20 ---\n","LR: 1.07e-04 | Batch: 16 | WD: 1.74e-04 | Dropout: 0.47 | Opt: adam | Momentum: 0.88 | Scheduler: steplr\n","EfficientNetB2 Trial 4, Epoch 1/10, Val Acc: 0.9766\n","EfficientNetB2 Trial 4, Epoch 2/10, Val Acc: 0.9845\n","EfficientNetB2 Trial 4, Epoch 3/10, Val Acc: 0.9856\n","EfficientNetB2 Trial 4, Epoch 4/10, Val Acc: 0.9856\n","EfficientNetB2 Trial 4, Epoch 5/10, Val Acc: 0.9867\n","EfficientNetB2 Trial 4, Epoch 6/10, Val Acc: 0.9924\n","EfficientNetB2 Trial 4, Epoch 7/10, Val Acc: 0.9921\n","EfficientNetB2 Trial 4, Epoch 8/10, Val Acc: 0.9929\n","EfficientNetB2 Trial 4, Epoch 9/10, Val Acc: 0.9910\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 23:18:19,169] Trial 3 finished with value: 0.9929405376052132 and parameters: {'learning_rate': 0.00010721889097027262, 'batch_size': 16, 'weight_decay': 0.00017356228723337791, 'dropout_rate': 0.47271036601116667, 'optimizer': 'adam', 'momentum': 0.8849672213618568, 'scheduler': 'steplr'}. Best is trial 0 with value: 0.9945696443117024.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 4, Epoch 10/10, Val Acc: 0.9921\n","\n","--- EfficientNetB2 Trial 5/20 ---\n","LR: 2.06e-05 | Batch: 16 | WD: 1.10e-04 | Dropout: 0.30 | Opt: sgd | Momentum: 0.93 | Scheduler: none\n","EfficientNetB2 Trial 5, Epoch 1/10, Val Acc: 0.6810\n","EfficientNetB2 Trial 5, Epoch 2/10, Val Acc: 0.7869\n","EfficientNetB2 Trial 5, Epoch 3/10, Val Acc: 0.8067\n","EfficientNetB2 Trial 5, Epoch 4/10, Val Acc: 0.8403\n","EfficientNetB2 Trial 5, Epoch 5/10, Val Acc: 0.8441\n","EfficientNetB2 Trial 5, Epoch 6/10, Val Acc: 0.8509\n","EfficientNetB2 Trial 5, Epoch 7/10, Val Acc: 0.8642\n","EfficientNetB2 Trial 5, Epoch 8/10, Val Acc: 0.8819\n","EfficientNetB2 Trial 5, Epoch 9/10, Val Acc: 0.8868\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 23:38:45,138] Trial 4 finished with value: 0.9011675264729839 and parameters: {'learning_rate': 2.063066539802646e-05, 'batch_size': 16, 'weight_decay': 0.00010977378442417086, 'dropout_rate': 0.30429173889984096, 'optimizer': 'sgd', 'momentum': 0.9344936856646906, 'scheduler': 'none'}. Best is trial 0 with value: 0.9945696443117024.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 5, Epoch 10/10, Val Acc: 0.9012\n","\n","--- EfficientNetB2 Trial 6/20 ---\n","LR: 2.42e-03 | Batch: 32 | WD: 2.55e-06 | Dropout: 0.28 | Opt: adam | Momentum: 0.62 | Scheduler: none\n","EfficientNetB2 Trial 6, Epoch 1/10, Val Acc: 0.9481\n","EfficientNetB2 Trial 6, Epoch 2/10, Val Acc: 0.9688\n","EfficientNetB2 Trial 6, Epoch 3/10, Val Acc: 0.9612\n","EfficientNetB2 Trial 6, Epoch 4/10, Val Acc: 0.9623\n","EfficientNetB2 Trial 6, Epoch 5/10, Val Acc: 0.9720\n","EfficientNetB2 Trial 6, Epoch 6/10, Val Acc: 0.9824\n","EfficientNetB2 Trial 6, Epoch 7/10, Val Acc: 0.9788\n","EfficientNetB2 Trial 6, Epoch 8/10, Val Acc: 0.9715\n","EfficientNetB2 Trial 6, Epoch 9/10, Val Acc: 0.9853\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 23:57:27,639] Trial 5 finished with value: 0.9853380396415965 and parameters: {'learning_rate': 0.0024245209345779323, 'batch_size': 32, 'weight_decay': 2.553825802214045e-06, 'dropout_rate': 0.28138414104618176, 'optimizer': 'adam', 'momentum': 0.6195697301370023, 'scheduler': 'none'}. Best is trial 0 with value: 0.9945696443117024.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 6, Epoch 10/10, Val Acc: 0.9777\n","\n","--- EfficientNetB2 Trial 7/20 ---\n","LR: 8.64e-05 | Batch: 16 | WD: 1.57e-05 | Dropout: 0.01 | Opt: sgd | Momentum: 0.53 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-06 23:59:37,350] Trial 6 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 7, Epoch 1/10, Val Acc: 0.5968\n","EfficientNetB2 Trial 7 pruned at epoch 1\n","\n","--- EfficientNetB2 Trial 8/20 ---\n","LR: 6.82e-03 | Batch: 32 | WD: 2.32e-04 | Dropout: 0.18 | Opt: adam | Momentum: 0.95 | Scheduler: none\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 00:01:36,024] Trial 7 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 8, Epoch 1/10, Val Acc: 0.6992\n","EfficientNetB2 Trial 8 pruned at epoch 1\n","\n","--- EfficientNetB2 Trial 9/20 ---\n","LR: 6.44e-05 | Batch: 32 | WD: 1.37e-05 | Dropout: 0.07 | Opt: sgd | Momentum: 0.95 | Scheduler: steplr\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 00:03:33,468] Trial 8 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 9, Epoch 1/10, Val Acc: 0.7931\n","EfficientNetB2 Trial 9 pruned at epoch 1\n","\n","--- EfficientNetB2 Trial 10/20 ---\n","LR: 1.21e-03 | Batch: 16 | WD: 4.10e-06 | Dropout: 0.05 | Opt: adam | Momentum: 0.52 | Scheduler: steplr\n","EfficientNetB2 Trial 10, Epoch 1/10, Val Acc: 0.9587\n","EfficientNetB2 Trial 10, Epoch 2/10, Val Acc: 0.9620\n","EfficientNetB2 Trial 10, Epoch 3/10, Val Acc: 0.9210\n","EfficientNetB2 Trial 10, Epoch 4/10, Val Acc: 0.9677\n","EfficientNetB2 Trial 10, Epoch 5/10, Val Acc: 0.9737\n","EfficientNetB2 Trial 10, Epoch 6/10, Val Acc: 0.9889\n","EfficientNetB2 Trial 10, Epoch 7/10, Val Acc: 0.9894\n","EfficientNetB2 Trial 10, Epoch 8/10, Val Acc: 0.9921\n","EfficientNetB2 Trial 10, Epoch 9/10, Val Acc: 0.9929\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 00:24:13,422] Trial 9 finished with value: 0.9929405376052132 and parameters: {'learning_rate': 0.0012148261380190081, 'batch_size': 16, 'weight_decay': 4.095559786123408e-06, 'dropout_rate': 0.04943397869669325, 'optimizer': 'adam', 'momentum': 0.5158346287831752, 'scheduler': 'steplr'}. Best is trial 0 with value: 0.9945696443117024.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 10, Epoch 10/10, Val Acc: 0.9900\n","\n","--- EfficientNetB2 Trial 11/20 ---\n","LR: 4.42e-04 | Batch: 64 | WD: 1.04e-06 | Dropout: 0.21 | Opt: adam | Momentum: 0.78 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 11, Epoch 1/10, Val Acc: 0.9794\n","EfficientNetB2 Trial 11, Epoch 2/10, Val Acc: 0.9853\n","EfficientNetB2 Trial 11, Epoch 3/10, Val Acc: 0.9886\n","EfficientNetB2 Trial 11, Epoch 4/10, Val Acc: 0.9905\n","EfficientNetB2 Trial 11, Epoch 5/10, Val Acc: 0.9910\n","EfficientNetB2 Trial 11, Epoch 6/10, Val Acc: 0.9943\n","EfficientNetB2 Trial 11, Epoch 7/10, Val Acc: 0.9938\n","EfficientNetB2 Trial 11, Epoch 8/10, Val Acc: 0.9951\n","EfficientNetB2 Trial 11, Epoch 9/10, Val Acc: 0.9973\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 00:42:19,685] Trial 10 finished with value: 0.9972848221558512 and parameters: {'learning_rate': 0.00044247491844838955, 'batch_size': 64, 'weight_decay': 1.0416492245370212e-06, 'dropout_rate': 0.2053446208826728, 'optimizer': 'adam', 'momentum': 0.7780488575314466, 'scheduler': 'cosineannealing'}. Best is trial 10 with value: 0.9972848221558512.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 11, Epoch 10/10, Val Acc: 0.9957\n","\n","--- EfficientNetB2 Trial 12/20 ---\n","LR: 4.24e-04 | Batch: 64 | WD: 2.03e-06 | Dropout: 0.17 | Opt: adam | Momentum: 0.78 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 12, Epoch 1/10, Val Acc: 0.9799\n","EfficientNetB2 Trial 12, Epoch 2/10, Val Acc: 0.9799\n","EfficientNetB2 Trial 12, Epoch 3/10, Val Acc: 0.9834\n","EfficientNetB2 Trial 12, Epoch 4/10, Val Acc: 0.9940\n","EfficientNetB2 Trial 12, Epoch 5/10, Val Acc: 0.9929\n","EfficientNetB2 Trial 12, Epoch 6/10, Val Acc: 0.9935\n","EfficientNetB2 Trial 12, Epoch 7/10, Val Acc: 0.9924\n","EfficientNetB2 Trial 12, Epoch 8/10, Val Acc: 0.9932\n","EfficientNetB2 Trial 12, Epoch 9/10, Val Acc: 0.9948\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 01:00:25,296] Trial 11 finished with value: 0.9948411620961173 and parameters: {'learning_rate': 0.00042448675631198934, 'batch_size': 64, 'weight_decay': 2.028409694113214e-06, 'dropout_rate': 0.1735805917487228, 'optimizer': 'adam', 'momentum': 0.7789892025869979, 'scheduler': 'cosineannealing'}. Best is trial 10 with value: 0.9972848221558512.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 12, Epoch 10/10, Val Acc: 0.9943\n","\n","--- EfficientNetB2 Trial 13/20 ---\n","LR: 3.64e-04 | Batch: 64 | WD: 1.21e-06 | Dropout: 0.20 | Opt: adam | Momentum: 0.79 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 13, Epoch 1/10, Val Acc: 0.9745\n","EfficientNetB2 Trial 13, Epoch 2/10, Val Acc: 0.9837\n","EfficientNetB2 Trial 13, Epoch 3/10, Val Acc: 0.9897\n","EfficientNetB2 Trial 13, Epoch 4/10, Val Acc: 0.9891\n","EfficientNetB2 Trial 13, Epoch 5/10, Val Acc: 0.9924\n","EfficientNetB2 Trial 13, Epoch 6/10, Val Acc: 0.9935\n","EfficientNetB2 Trial 13, Epoch 7/10, Val Acc: 0.9943\n","EfficientNetB2 Trial 13, Epoch 8/10, Val Acc: 0.9957\n","EfficientNetB2 Trial 13, Epoch 9/10, Val Acc: 0.9948\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 01:18:33,024] Trial 12 finished with value: 0.9956557154493619 and parameters: {'learning_rate': 0.0003637563309425318, 'batch_size': 64, 'weight_decay': 1.2054299444447356e-06, 'dropout_rate': 0.19527439793647053, 'optimizer': 'adam', 'momentum': 0.7885054582581612, 'scheduler': 'cosineannealing'}. Best is trial 10 with value: 0.9972848221558512.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 13, Epoch 10/10, Val Acc: 0.9943\n","\n","--- EfficientNetB2 Trial 14/20 ---\n","LR: 3.51e-04 | Batch: 64 | WD: 1.08e-06 | Dropout: 0.38 | Opt: adam | Momentum: 0.80 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 14, Epoch 1/10, Val Acc: 0.9802\n","EfficientNetB2 Trial 14, Epoch 2/10, Val Acc: 0.9845\n","EfficientNetB2 Trial 14, Epoch 3/10, Val Acc: 0.9848\n","EfficientNetB2 Trial 14, Epoch 4/10, Val Acc: 0.9905\n","EfficientNetB2 Trial 14, Epoch 5/10, Val Acc: 0.9910\n","EfficientNetB2 Trial 14, Epoch 6/10, Val Acc: 0.9910\n","EfficientNetB2 Trial 14, Epoch 7/10, Val Acc: 0.9913\n","EfficientNetB2 Trial 14, Epoch 8/10, Val Acc: 0.9932\n","EfficientNetB2 Trial 14, Epoch 9/10, Val Acc: 0.9921\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 01:36:38,608] Trial 13 finished with value: 0.993212055389628 and parameters: {'learning_rate': 0.0003511183379630398, 'batch_size': 64, 'weight_decay': 1.0814396746405406e-06, 'dropout_rate': 0.3770216768382489, 'optimizer': 'adam', 'momentum': 0.8026487070065169, 'scheduler': 'cosineannealing'}. Best is trial 10 with value: 0.9972848221558512.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 14, Epoch 10/10, Val Acc: 0.9927\n","\n","--- EfficientNetB2 Trial 15/20 ---\n","LR: 2.09e-05 | Batch: 64 | WD: 6.58e-06 | Dropout: 0.17 | Opt: adam | Momentum: 0.71 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 01:38:32,327] Trial 14 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 15, Epoch 1/10, Val Acc: 0.8854\n","EfficientNetB2 Trial 15 pruned at epoch 1\n","\n","--- EfficientNetB2 Trial 16/20 ---\n","LR: 2.45e-04 | Batch: 64 | WD: 1.15e-06 | Dropout: 0.24 | Opt: adam | Momentum: 0.83 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 16, Epoch 1/10, Val Acc: 0.9794\n","EfficientNetB2 Trial 16, Epoch 2/10, Val Acc: 0.9878\n","EfficientNetB2 Trial 16, Epoch 3/10, Val Acc: 0.9837\n","EfficientNetB2 Trial 16, Epoch 4/10, Val Acc: 0.9924\n","EfficientNetB2 Trial 16, Epoch 5/10, Val Acc: 0.9881\n","EfficientNetB2 Trial 16, Epoch 6/10, Val Acc: 0.9924\n","EfficientNetB2 Trial 16, Epoch 7/10, Val Acc: 0.9943\n","EfficientNetB2 Trial 16, Epoch 8/10, Val Acc: 0.9916\n","EfficientNetB2 Trial 16, Epoch 9/10, Val Acc: 0.9935\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 01:56:29,243] Trial 15 finished with value: 0.9942981265272876 and parameters: {'learning_rate': 0.0002447705295735127, 'batch_size': 64, 'weight_decay': 1.1474507415824236e-06, 'dropout_rate': 0.2384109929477773, 'optimizer': 'adam', 'momentum': 0.8339297379060978, 'scheduler': 'cosineannealing'}. Best is trial 10 with value: 0.9972848221558512.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 16, Epoch 10/10, Val Acc: 0.9935\n","\n","--- EfficientNetB2 Trial 17/20 ---\n","LR: 6.13e-04 | Batch: 64 | WD: 5.98e-06 | Dropout: 0.36 | Opt: adam | Momentum: 0.67 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 17, Epoch 1/10, Val Acc: 0.9783\n","EfficientNetB2 Trial 17, Epoch 2/10, Val Acc: 0.9902\n","EfficientNetB2 Trial 17, Epoch 3/10, Val Acc: 0.9870\n","EfficientNetB2 Trial 17, Epoch 4/10, Val Acc: 0.9921\n","EfficientNetB2 Trial 17, Epoch 5/10, Val Acc: 0.9943\n","EfficientNetB2 Trial 17, Epoch 6/10, Val Acc: 0.9957\n","EfficientNetB2 Trial 17, Epoch 7/10, Val Acc: 0.9967\n","EfficientNetB2 Trial 17, Epoch 8/10, Val Acc: 0.9957\n","EfficientNetB2 Trial 17, Epoch 9/10, Val Acc: 0.9962\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 02:14:33,372] Trial 16 finished with value: 0.9972848221558512 and parameters: {'learning_rate': 0.0006128609518137366, 'batch_size': 64, 'weight_decay': 5.977541931340391e-06, 'dropout_rate': 0.3584571936957548, 'optimizer': 'adam', 'momentum': 0.6679733941477679, 'scheduler': 'cosineannealing'}. Best is trial 10 with value: 0.9972848221558512.\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 17, Epoch 10/10, Val Acc: 0.9973\n","\n","--- EfficientNetB2 Trial 18/20 ---\n","LR: 4.73e-03 | Batch: 64 | WD: 7.08e-06 | Dropout: 0.37 | Opt: adam | Momentum: 0.67 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 02:16:27,709] Trial 17 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 18, Epoch 1/10, Val Acc: 0.8257\n","EfficientNetB2 Trial 18 pruned at epoch 1\n","\n","--- EfficientNetB2 Trial 19/20 ---\n","LR: 7.23e-04 | Batch: 64 | WD: 4.16e-05 | Dropout: 0.46 | Opt: adam | Momentum: 0.68 | Scheduler: cosineannealing\n","EfficientNetB2 Trial 19, Epoch 1/10, Val Acc: 0.9813\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 02:20:07,792] Trial 18 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 19, Epoch 2/10, Val Acc: 0.9709\n","EfficientNetB2 Trial 19 pruned at epoch 2\n","\n","--- EfficientNetB2 Trial 20/20 ---\n","LR: 2.15e-03 | Batch: 64 | WD: 6.68e-06 | Dropout: 0.39 | Opt: adam | Momentum: 0.59 | Scheduler: steplr\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 02:22:02,562] Trial 19 pruned. \n","[I 2025-06-07 02:22:02,571] A new study created in memory with name: no-name-c8fa8440-db84-4af7-b52b-f07522c8823f\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNetB2 Trial 20, Epoch 1/10, Val Acc: 0.9568\n","EfficientNetB2 Trial 20 pruned at epoch 1\n","\n","===== Optimizing MobileNetV3 =====\n","\n","--- MobileNetV3 Trial 1/20 ---\n","LR: 7.24e-05 | Batch: 16 | WD: 1.94e-06 | Dropout: 0.44 | Opt: adam | Momentum: 0.93 | Scheduler: cosineannealing\n","MobileNetV3 Trial 1, Epoch 1/10, Val Acc: 0.9457\n","MobileNetV3 Trial 1, Epoch 2/10, Val Acc: 0.9764\n","MobileNetV3 Trial 1, Epoch 3/10, Val Acc: 0.9775\n","MobileNetV3 Trial 1, Epoch 4/10, Val Acc: 0.9796\n","MobileNetV3 Trial 1, Epoch 5/10, Val Acc: 0.9832\n","MobileNetV3 Trial 1, Epoch 6/10, Val Acc: 0.9862\n","MobileNetV3 Trial 1, Epoch 7/10, Val Acc: 0.9840\n","MobileNetV3 Trial 1, Epoch 8/10, Val Acc: 0.9881\n","MobileNetV3 Trial 1, Epoch 9/10, Val Acc: 0.9870\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 02:37:43,084] Trial 0 finished with value: 0.9880532174857453 and parameters: {'learning_rate': 7.243209856501207e-05, 'batch_size': 16, 'weight_decay': 1.936116092420596e-06, 'dropout_rate': 0.43754266390824964, 'optimizer': 'adam', 'momentum': 0.933519928739075, 'scheduler': 'cosineannealing'}. Best is trial 0 with value: 0.9880532174857453.\n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 1, Epoch 10/10, Val Acc: 0.9875\n","\n","--- MobileNetV3 Trial 2/20 ---\n","LR: 9.28e-04 | Batch: 32 | WD: 6.22e-05 | Dropout: 0.36 | Opt: adam | Momentum: 0.53 | Scheduler: steplr\n","MobileNetV3 Trial 2, Epoch 1/10, Val Acc: 0.8955\n","MobileNetV3 Trial 2, Epoch 2/10, Val Acc: 0.8881\n","MobileNetV3 Trial 2, Epoch 3/10, Val Acc: 0.9166\n","MobileNetV3 Trial 2, Epoch 4/10, Val Acc: 0.9519\n","MobileNetV3 Trial 2, Epoch 5/10, Val Acc: 0.8803\n","MobileNetV3 Trial 2, Epoch 6/10, Val Acc: 0.9870\n","MobileNetV3 Trial 2, Epoch 7/10, Val Acc: 0.9881\n","MobileNetV3 Trial 2, Epoch 8/10, Val Acc: 0.9886\n","MobileNetV3 Trial 2, Epoch 9/10, Val Acc: 0.9897\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 02:51:56,083] Trial 1 finished with value: 0.9896823241922346 and parameters: {'learning_rate': 0.0009276066182252605, 'batch_size': 32, 'weight_decay': 6.217606355005425e-05, 'dropout_rate': 0.35519897402548045, 'optimizer': 'adam', 'momentum': 0.5325479242722393, 'scheduler': 'steplr'}. Best is trial 1 with value: 0.9896823241922346.\n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 2, Epoch 10/10, Val Acc: 0.9897\n","\n","--- MobileNetV3 Trial 3/20 ---\n","LR: 7.81e-04 | Batch: 16 | WD: 4.51e-05 | Dropout: 0.14 | Opt: sgd | Momentum: 0.78 | Scheduler: steplr\n","MobileNetV3 Trial 3, Epoch 1/10, Val Acc: 0.8781\n","MobileNetV3 Trial 3, Epoch 2/10, Val Acc: 0.9400\n","MobileNetV3 Trial 3, Epoch 3/10, Val Acc: 0.9525\n","MobileNetV3 Trial 3, Epoch 4/10, Val Acc: 0.9335\n","MobileNetV3 Trial 3, Epoch 5/10, Val Acc: 0.9704\n","MobileNetV3 Trial 3, Epoch 6/10, Val Acc: 0.9707\n","MobileNetV3 Trial 3, Epoch 7/10, Val Acc: 0.9701\n","MobileNetV3 Trial 3, Epoch 8/10, Val Acc: 0.9723\n","MobileNetV3 Trial 3, Epoch 9/10, Val Acc: 0.9728\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:07:27,637] Trial 2 finished with value: 0.972848221558512 and parameters: {'learning_rate': 0.0007805757153744992, 'batch_size': 16, 'weight_decay': 4.513068489871641e-05, 'dropout_rate': 0.13577939613354062, 'optimizer': 'sgd', 'momentum': 0.7790440280355433, 'scheduler': 'steplr'}. Best is trial 1 with value: 0.9896823241922346.\n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 3, Epoch 10/10, Val Acc: 0.9723\n","\n","--- MobileNetV3 Trial 4/20 ---\n","LR: 1.93e-03 | Batch: 16 | WD: 5.97e-06 | Dropout: 0.30 | Opt: sgd | Momentum: 0.58 | Scheduler: cosineannealing\n","MobileNetV3 Trial 4, Epoch 1/10, Val Acc: 0.9069\n","MobileNetV3 Trial 4, Epoch 2/10, Val Acc: 0.9365\n","MobileNetV3 Trial 4, Epoch 3/10, Val Acc: 0.9490\n","MobileNetV3 Trial 4, Epoch 4/10, Val Acc: 0.9471\n","MobileNetV3 Trial 4, Epoch 5/10, Val Acc: 0.9639\n","MobileNetV3 Trial 4, Epoch 6/10, Val Acc: 0.9696\n","MobileNetV3 Trial 4, Epoch 7/10, Val Acc: 0.9693\n","MobileNetV3 Trial 4, Epoch 8/10, Val Acc: 0.9720\n","MobileNetV3 Trial 4, Epoch 9/10, Val Acc: 0.9726\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:22:50,803] Trial 3 finished with value: 0.9725767037740972 and parameters: {'learning_rate': 0.0019340786329383915, 'batch_size': 16, 'weight_decay': 5.965725035820939e-06, 'dropout_rate': 0.30365088033573145, 'optimizer': 'sgd', 'momentum': 0.5790728426887964, 'scheduler': 'cosineannealing'}. Best is trial 1 with value: 0.9896823241922346.\n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 4, Epoch 10/10, Val Acc: 0.9726\n","\n","--- MobileNetV3 Trial 5/20 ---\n","LR: 1.84e-05 | Batch: 64 | WD: 4.48e-05 | Dropout: 0.47 | Opt: sgd | Momentum: 0.67 | Scheduler: steplr\n","MobileNetV3 Trial 5, Epoch 1/10, Val Acc: 0.2506\n","MobileNetV3 Trial 5, Epoch 2/10, Val Acc: 0.2759\n","MobileNetV3 Trial 5, Epoch 3/10, Val Acc: 0.3820\n","MobileNetV3 Trial 5, Epoch 4/10, Val Acc: 0.5018\n","MobileNetV3 Trial 5, Epoch 5/10, Val Acc: 0.5707\n","MobileNetV3 Trial 5, Epoch 6/10, Val Acc: 0.5729\n","MobileNetV3 Trial 5, Epoch 7/10, Val Acc: 0.5781\n","MobileNetV3 Trial 5, Epoch 8/10, Val Acc: 0.5832\n","MobileNetV3 Trial 5, Epoch 9/10, Val Acc: 0.5873\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:36:07,594] Trial 4 finished with value: 0.5943524300841705 and parameters: {'learning_rate': 1.837729415278907e-05, 'batch_size': 64, 'weight_decay': 4.479517474998775e-05, 'dropout_rate': 0.4699966138529616, 'optimizer': 'sgd', 'momentum': 0.667680185401558, 'scheduler': 'steplr'}. Best is trial 1 with value: 0.9896823241922346.\n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 5, Epoch 10/10, Val Acc: 0.5944\n","\n","--- MobileNetV3 Trial 6/20 ---\n","LR: 1.37e-05 | Batch: 16 | WD: 6.15e-04 | Dropout: 0.07 | Opt: sgd | Momentum: 0.81 | Scheduler: steplr\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:37:48,437] Trial 5 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 6, Epoch 1/10, Val Acc: 0.5558\n","MobileNetV3 Trial 6 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 7/20 ---\n","LR: 1.57e-05 | Batch: 16 | WD: 1.18e-04 | Dropout: 0.26 | Opt: adam | Momentum: 0.82 | Scheduler: steplr\n","MobileNetV3 Trial 7, Epoch 1/10, Val Acc: 0.9012\n","MobileNetV3 Trial 7, Epoch 2/10, Val Acc: 0.9395\n","MobileNetV3 Trial 7, Epoch 3/10, Val Acc: 0.9514\n","MobileNetV3 Trial 7, Epoch 4/10, Val Acc: 0.9585\n","MobileNetV3 Trial 7, Epoch 5/10, Val Acc: 0.9688\n","MobileNetV3 Trial 7, Epoch 6/10, Val Acc: 0.9726\n","MobileNetV3 Trial 7, Epoch 7/10, Val Acc: 0.9731\n","MobileNetV3 Trial 7, Epoch 8/10, Val Acc: 0.9723\n","MobileNetV3 Trial 7, Epoch 9/10, Val Acc: 0.9720\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:53:24,402] Trial 6 finished with value: 0.973119739342927 and parameters: {'learning_rate': 1.5663698281295747e-05, 'batch_size': 16, 'weight_decay': 0.00011783237737158422, 'dropout_rate': 0.26192878887831306, 'optimizer': 'adam', 'momentum': 0.8156951878024037, 'scheduler': 'steplr'}. Best is trial 1 with value: 0.9896823241922346.\n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 7, Epoch 10/10, Val Acc: 0.9715\n","\n","--- MobileNetV3 Trial 8/20 ---\n","LR: 8.46e-05 | Batch: 64 | WD: 3.47e-06 | Dropout: 0.48 | Opt: adam | Momentum: 0.71 | Scheduler: steplr\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:54:52,171] Trial 7 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 8, Epoch 1/10, Val Acc: 0.7187\n","MobileNetV3 Trial 8 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 9/20 ---\n","LR: 1.05e-05 | Batch: 16 | WD: 1.50e-05 | Dropout: 0.05 | Opt: adam | Momentum: 0.55 | Scheduler: none\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:56:35,792] Trial 8 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 9, Epoch 1/10, Val Acc: 0.8982\n","MobileNetV3 Trial 9 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 10/20 ---\n","LR: 2.31e-03 | Batch: 64 | WD: 6.56e-04 | Dropout: 0.13 | Opt: sgd | Momentum: 0.86 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:58:05,613] Trial 9 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 10, Epoch 1/10, Val Acc: 0.8197\n","MobileNetV3 Trial 10 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 11/20 ---\n","LR: 2.76e-04 | Batch: 32 | WD: 1.67e-04 | Dropout: 0.36 | Opt: adam | Momentum: 0.63 | Scheduler: none\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 03:59:37,359] Trial 10 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 11, Epoch 1/10, Val Acc: 0.8602\n","MobileNetV3 Trial 11 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 12/20 ---\n","LR: 8.68e-03 | Batch: 32 | WD: 1.39e-06 | Dropout: 0.39 | Opt: adam | Momentum: 0.95 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:01:10,171] Trial 11 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 12, Epoch 1/10, Val Acc: 0.4912\n","MobileNetV3 Trial 12 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 13/20 ---\n","LR: 1.60e-04 | Batch: 32 | WD: 1.22e-05 | Dropout: 0.38 | Opt: adam | Momentum: 0.95 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:02:42,049] Trial 12 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 13, Epoch 1/10, Val Acc: 0.8170\n","MobileNetV3 Trial 13 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 14/20 ---\n","LR: 8.42e-05 | Batch: 32 | WD: 1.00e-06 | Dropout: 0.31 | Opt: adam | Momentum: 0.50 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:04:14,627] Trial 13 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 14, Epoch 1/10, Val Acc: 0.6258\n","MobileNetV3 Trial 14 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 15/20 ---\n","LR: 5.62e-04 | Batch: 32 | WD: 1.31e-04 | Dropout: 0.43 | Opt: adam | Momentum: 0.87 | Scheduler: none\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:05:47,831] Trial 14 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 15, Epoch 1/10, Val Acc: 0.8678\n","MobileNetV3 Trial 15 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 16/20 ---\n","LR: 4.18e-05 | Batch: 16 | WD: 2.11e-05 | Dropout: 0.20 | Opt: adam | Momentum: 0.98 | Scheduler: steplr\n","MobileNetV3 Trial 16, Epoch 1/10, Val Acc: 0.9335\n","MobileNetV3 Trial 16, Epoch 2/10, Val Acc: 0.9685\n","MobileNetV3 Trial 16, Epoch 3/10, Val Acc: 0.9769\n","MobileNetV3 Trial 16, Epoch 4/10, Val Acc: 0.9764\n","MobileNetV3 Trial 16, Epoch 5/10, Val Acc: 0.9807\n","MobileNetV3 Trial 16, Epoch 6/10, Val Acc: 0.9813\n","MobileNetV3 Trial 16, Epoch 7/10, Val Acc: 0.9802\n","MobileNetV3 Trial 16, Epoch 8/10, Val Acc: 0.9810\n","MobileNetV3 Trial 16, Epoch 9/10, Val Acc: 0.9802\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:21:38,152] Trial 15 finished with value: 0.9823513440130328 and parameters: {'learning_rate': 4.179259364983111e-05, 'batch_size': 16, 'weight_decay': 2.1142488475055928e-05, 'dropout_rate': 0.2040230752697766, 'optimizer': 'adam', 'momentum': 0.982685569771224, 'scheduler': 'steplr'}. Best is trial 1 with value: 0.9896823241922346.\n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 16, Epoch 10/10, Val Acc: 0.9824\n","\n","--- MobileNetV3 Trial 17/20 ---\n","LR: 1.19e-03 | Batch: 32 | WD: 5.09e-06 | Dropout: 0.34 | Opt: adam | Momentum: 0.73 | Scheduler: cosineannealing\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:23:08,248] Trial 16 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 17, Epoch 1/10, Val Acc: 0.8349\n","MobileNetV3 Trial 17 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 18/20 ---\n","LR: 5.04e-03 | Batch: 32 | WD: 2.92e-04 | Dropout: 0.43 | Opt: adam | Momentum: 0.62 | Scheduler: steplr\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:24:41,406] Trial 17 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 18, Epoch 1/10, Val Acc: 0.3570\n","MobileNetV3 Trial 18 pruned at epoch 1\n","\n","--- MobileNetV3 Trial 19/20 ---\n","LR: 3.05e-04 | Batch: 16 | WD: 6.14e-05 | Dropout: 0.23 | Opt: adam | Momentum: 0.86 | Scheduler: cosineannealing\n","MobileNetV3 Trial 19, Epoch 1/10, Val Acc: 0.9226\n","MobileNetV3 Trial 19, Epoch 2/10, Val Acc: 0.9430\n","MobileNetV3 Trial 19, Epoch 3/10, Val Acc: 0.9756\n","MobileNetV3 Trial 19, Epoch 4/10, Val Acc: 0.9794\n","MobileNetV3 Trial 19, Epoch 5/10, Val Acc: 0.9758\n","MobileNetV3 Trial 19, Epoch 6/10, Val Acc: 0.9891\n","MobileNetV3 Trial 19, Epoch 7/10, Val Acc: 0.9859\n","MobileNetV3 Trial 19, Epoch 8/10, Val Acc: 0.9913\n","MobileNetV3 Trial 19, Epoch 9/10, Val Acc: 0.9902\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:40:35,936] Trial 18 finished with value: 0.9923975020363833 and parameters: {'learning_rate': 0.0003048410154682217, 'batch_size': 16, 'weight_decay': 6.13721716330716e-05, 'dropout_rate': 0.22750422326843966, 'optimizer': 'adam', 'momentum': 0.8569593846294994, 'scheduler': 'cosineannealing'}. Best is trial 18 with value: 0.9923975020363833.\n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 19, Epoch 10/10, Val Acc: 0.9924\n","\n","--- MobileNetV3 Trial 20/20 ---\n","LR: 3.18e-04 | Batch: 64 | WD: 6.50e-05 | Dropout: 0.22 | Opt: adam | Momentum: 0.88 | Scheduler: none\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-07 04:42:03,674] Trial 19 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["MobileNetV3 Trial 20, Epoch 1/10, Val Acc: 0.7912\n","MobileNetV3 Trial 20 pruned at epoch 1\n","\n","=== Top 3 Configurations per Model ===\n","            model  rank  learning_rate  batch_size  weight_decay  \\\n","0        ResNet50     1       0.000097          16      0.000070   \n","1        ResNet50     2       0.000077          32      0.000004   \n","2        ResNet50     3       0.003132          16      0.000486   \n","3  EfficientNetB2     1       0.000613          64      0.000006   \n","4  EfficientNetB2     2       0.000442          64      0.000001   \n","5  EfficientNetB2     3       0.000364          64      0.000001   \n","6     MobileNetV3     1       0.000305          16      0.000061   \n","7     MobileNetV3     2       0.000928          32      0.000062   \n","8     MobileNetV3     3       0.000072          16      0.000002   \n","\n","   dropout_rate optimizer  momentum        scheduler  val_accuracy  \n","0      0.403693      adam  0.534533  cosineannealing      0.994027  \n","1      0.484002      adam  0.752186  cosineannealing      0.990497  \n","2      0.317445       sgd  0.876210  cosineannealing      0.990225  \n","3      0.358457      adam  0.667973  cosineannealing      0.997285  \n","4      0.205345      adam  0.778049  cosineannealing      0.997285  \n","5      0.195274      adam  0.788505  cosineannealing      0.995656  \n","6      0.227504      adam  0.856959  cosineannealing      0.992398  \n","7      0.355199      adam  0.532548           steplr      0.989682  \n","8      0.437543      adam  0.933520  cosineannealing      0.988053  \n","\n","Summary saved to: /content/drive/MyDrive/Brandon's FYP/hparam_stage3_summary.csv\n"]}]},{"cell_type":"markdown","source":["# 3. Model Testing"],"metadata":{"id":"Il29TupBBG-t"}},{"cell_type":"markdown","source":["## a. Model Evaluation Function\n","\n","\n"],"metadata":{"id":"QJ3_cP2XKcy-"}},{"cell_type":"code","source":["def evaluate_model(model_name, run_version, model_filename):\n","    print(f\"\\n--- Evaluating {model_name} ({run_version}) ---\")\n","\n","    # --- Config & paths ---\n","    DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    BASE_DIR   = \"/content/drive/MyDrive/Brandon's FYP\"\n","    MODEL_PATH = os.path.join(BASE_DIR, model_name, run_version, model_filename)\n","    SAVE_DIR   = os.path.dirname(MODEL_PATH)\n","    TEST_IMG_DIR = \"data/test\"\n","    TEST_CSV     = os.path.join(TEST_IMG_DIR, \"_classes.csv\")\n","    os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","    # --- Load CSV and prepare classes ---\n","    df = pd.read_csv(TEST_CSV)\n","    df.columns = df.columns.str.strip()\n","    class_cols = [c for c in df.columns if c != 'filename']\n","    NUM_CLASSES = len(class_cols)\n","\n","    # --- Load model architecture and weights ---\n","    models_dict = dict(get_all_models(NUM_CLASSES))\n","    model = models_dict[model_name]\n","    state = torch.load(MODEL_PATH, map_location=DEVICE)\n","    model.load_state_dict(state)\n","    model = model.to(DEVICE)\n","    model.eval()\n","\n","    # --- Transforms (match validation pipeline) ---\n","    transform = T.Compose([\n","        T.Resize((224,224)),\n","        T.ToTensor(),\n","        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","    ])\n","\n","    # --- Inference loop ---\n","    y_true, y_pred, y_prob = [], [], []\n","    with torch.no_grad():\n","        for _, row in df.iterrows():\n","            img_path = os.path.join(TEST_IMG_DIR, row['filename'].strip())\n","            img = Image.open(img_path).convert(\"RGB\")\n","            x   = transform(img).unsqueeze(0).to(DEVICE)\n","            logits = model(x)\n","            probs  = torch.softmax(logits, dim=1).cpu().numpy()[0]\n","            pred   = int(np.argmax(probs))\n","            true   = int(row[class_cols].astype(int).values.argmax())\n","            y_true.append(true)\n","            y_pred.append(pred)\n","            y_prob.append(probs)\n","    y_prob = np.array(y_prob)\n","\n","    # --- Confusion Matrix ---\n","    cm = confusion_matrix(y_true, y_pred)\n","    cm_norm = cm.astype(float) / cm.sum(axis=1)[:, None]\n","    def plot_cm(matrix, title, fname, norm=False):\n","        fig, ax = plt.subplots()\n","        im = ax.imshow(matrix, cmap='Blues', vmin=0 if norm else None, vmax=1 if norm else None)\n","        plt.colorbar(im, ax=ax)\n","        ax.set(xticks=np.arange(NUM_CLASSES), yticks=np.arange(NUM_CLASSES),\n","               xticklabels=class_cols, yticklabels=class_cols,\n","               xlabel='Predicted', ylabel='Actual', title=title)\n","        for i in range(NUM_CLASSES):\n","            for j in range(NUM_CLASSES):\n","                value = f\"{matrix[i,j]:.2f}\" if norm else str(matrix[i,j])\n","                ax.text(j, i, value, ha='center', va='center',\n","                        color='white' if matrix[i,j] > matrix.max()/2. else 'black')\n","        fig.tight_layout()\n","        fig.savefig(os.path.join(SAVE_DIR, fname), dpi=150)\n","        plt.close(fig)\n","    plot_cm(cm, f'{model_name}: Confusion Matrix', 'confusion_matrix.png')\n","    plot_cm(cm_norm, f'{model_name}: Normalized Confusion Matrix', 'confusion_matrix_normalized.png', norm=True)\n","\n","    # --- F1 Curve ---\n","    common_t = np.linspace(0,1,100)\n","    f1_curves = []\n","    for idx, cls in enumerate(class_cols):\n","        yb      = np.array(y_true) == idx\n","        scores  = y_prob[:, idx]\n","        prec, rec, th = precision_recall_curve(yb, scores)\n","        f1_vals = 2 * prec * rec / (prec + rec + 1e-8)\n","        th_ext  = np.concatenate(([0], th))\n","        f1_curves.append(np.interp(common_t, th_ext, f1_vals))\n","    mean_f1 = np.mean(f1_curves, axis=0)\n","    fig, ax = plt.subplots()\n","    for idx, cls in enumerate(class_cols):\n","        ax.plot(common_t, f1_curves[idx], label=cls)\n","    ax.plot(common_t, mean_f1, 'k--', lw=2, label='Average')\n","    ax.set(title=f'{model_name}: F1 Score vs. Threshold', xlabel='Threshold', ylabel='F1 Score')\n","    ax.legend(loc='lower left', fontsize='small')\n","    fig.tight_layout()\n","    fig.savefig(os.path.join(SAVE_DIR, 'f1_curve.png'), dpi=150)\n","    plt.close(fig)\n","\n","    # --- ROC Curve ---\n","    y_true_bin = label_binarize(y_true, classes=list(range(NUM_CLASSES)))\n","    fpr, tpr, roc_auc = {}, {}, {}\n","    for i in range(NUM_CLASSES):\n","        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","    fig, ax = plt.subplots()\n","    for i, cls in enumerate(class_cols):\n","        ax.plot(fpr[i], tpr[i], label=f'{cls} (AUC = {roc_auc[i]:.2f})')\n","    ax.plot([0,1], [0,1], 'k--', linewidth=1)\n","    ax.set(xlabel='False Positive Rate', ylabel='True Positive Rate',\n","           title=f'{model_name}: ROC Curves (OvR)')\n","    ax.legend(loc='lower right', fontsize='small')\n","    fig.tight_layout()\n","    fig.savefig(os.path.join(SAVE_DIR, 'roc_curve.png'), dpi=150)\n","    plt.close(fig)\n","\n","    # --- Classification Report ---\n","    report = classification_report(y_true, y_pred, target_names=class_cols, output_dict=True, zero_division=0)\n","    df_report = pd.DataFrame(report).T\n","    df_report.to_csv(os.path.join(SAVE_DIR, 'classification_report.csv'))\n","\n","    fig, ax = plt.subplots(figsize=(8, 1 + 0.5*len(df_report)))\n","    ax.axis('off')\n","    tbl = ax.table(\n","        cellText=np.round(df_report.values, 3),\n","        rowLabels=df_report.index,\n","        colLabels=df_report.columns,\n","        cellLoc='center', loc='center'\n","    )\n","    tbl.auto_set_font_size(False)\n","    tbl.set_fontsize(10)\n","    tbl.scale(1, 1.5)\n","    plt.title(f'{model_name}: Classification Report', pad=20)\n","    fig.tight_layout()\n","    fig.savefig(os.path.join(SAVE_DIR, 'classification_report.png'), dpi=150)\n","    plt.close(fig)\n","\n","    print(f\"✅ Saved all evaluation outputs for {model_name} to:\\n → {SAVE_DIR}\")"],"metadata":{"id":"x9HRlZR9KgKC","executionInfo":{"status":"ok","timestamp":1749487812028,"user_tz":-480,"elapsed":21,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## b. CPU Timing Function"],"metadata":{"id":"IHhOfbEO2vzn"}},{"cell_type":"code","source":["def measure_cpu_inference_time(\n","    test_csv,\n","    test_img_dir,\n","    excel_path,\n","    sheet_name=\"Top 3 Configurations\",\n","    output_csv_path=\"/content/drive/MyDrive/Brandon's FYP/cpu_inference_time_full.csv\",\n","    batch_size=32,\n","    device=torch.device(\"cpu\")\n","):\n","\n","    print(\"\\n=== CPU Inference Measurement START ===\")\n","    print(f\"Loading top‐1 configs from '{excel_path}' (sheet '{sheet_name}')…\")\n","    # 1. Load top-1 configs\n","    df_top3_direct = pd.read_excel(excel_path, sheet_name=sheet_name, header=0)\n","    df_top1_direct = df_top3_direct[df_top3_direct[\"Rank\"] == 1]\n","    print(f\"Found {len(df_top1_direct)} top-1 configurations (one per model).\")\n","\n","    # 2. Build test DataLoader\n","    print(f\"Building test DataLoader from CSV: '{test_csv}' and image dir: '{test_img_dir}'…\")\n","    _, val_transform = get_transforms(mode=1)\n","    test_ds = CSVClassificationDataset(test_csv, test_img_dir, transform=val_transform)\n","    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n","    num_images = len(test_ds)\n","    num_classes = test_ds.num_classes\n","    print(f\"Test set contains {num_images} images, {num_classes} classes.\")\n","\n","    results = []\n","    # 3. Measure for each model\n","    for _, row in df_top1_direct.iterrows():\n","        model_name = row[\"Model\"]\n","        dropout_rate = row[\"Dropout\"]\n","        print(f\"\\n*** Evaluating model: {model_name} ***\")\n","\n","        # instantiate model\n","        if model_name == \"ResNet50\":\n","            m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n","            m.fc = nn.Sequential(nn.Dropout(dropout_rate),\n","                                 nn.Linear(m.fc.in_features, num_classes))\n","        elif model_name == \"EfficientNetB2\":\n","            m = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.IMAGENET1K_V1)\n","            m.classifier[1] = nn.Sequential(nn.Dropout(dropout_rate),\n","                                         nn.Linear(m.classifier[1].in_features, num_classes))\n","        elif model_name == \"MobileNetV3\":\n","            m = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n","            m.classifier[3] = nn.Sequential(nn.Dropout(dropout_rate),\n","                                         nn.Linear(m.classifier[3].in_features, num_classes))\n","        else:\n","            print(f\"  [Warning] Unknown model '{model_name}', skipping.\")\n","            continue\n","\n","        m.to(device)\n","        m.eval()\n","\n","        # Warm-up\n","        print(\"  Warm-up pass…\")\n","        with torch.no_grad():\n","            imgs, _ = next(iter(test_loader))\n","            _ = m(imgs.to(device))\n","\n","        # Timed inference\n","        print(f\"  Timing inference on {num_images} images (batch_size={batch_size})…\")\n","        start = perf_counter()\n","        with torch.no_grad():\n","            for imgs, _ in test_loader:\n","                _ = m(imgs.to(device))\n","        end = perf_counter()\n","\n","        elapsed = end - start\n","        print(f\"  ► {model_name} CPU inference time: {elapsed:.2f} s\")\n","\n","        results.append({\n","            \"model\": model_name,\n","            \"cpu_inference_time_s\": elapsed,\n","            \"num_images\": num_images,\n","            \"val_accuracy\": row[\"Val Accuracy\"]\n","        })\n","\n","    # 4. Output\n","    df_time = pd.DataFrame(results)\n","    print(\"\\n=== All models evaluated ===\")\n","    print(df_time)\n","\n","    print(f\"Exporting results to '{output_csv_path}'…\")\n","    df_time.to_csv(output_csv_path, index=False)\n","    print(\"Saved CPU timing results successfully.\")\n","    print(\"=== CPU Inference Measurement END ===\\n\")\n"],"metadata":{"id":"Zl0XwpsW28LX","executionInfo":{"status":"ok","timestamp":1749490589721,"user_tz":-480,"elapsed":32,"user":{"displayName":"FYP","userId":"06650288711143266396"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["# Main Testing Launcher"],"metadata":{"id":"muwlHeoCK7Zn"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # for model_name in [\"ResNet50\", \"EfficientNetB2\", \"MobileNetV3\"]:\n","    #     evaluate_model(model_name, run_version=\"run_v3\", model_filename=\"best.pth\")\n","    measure_cpu_inference_time(\n","        test_csv=TEST_CSV,\n","        test_img_dir=TEST_IMG_DIR,\n","        excel_path=OPTIMIZED_HYPERPARAMETERS\n","    )"],"metadata":{"id":"i0U4ZGrlTmjF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749491493255,"user_tz":-480,"elapsed":901134,"user":{"displayName":"FYP","userId":"06650288711143266396"}},"outputId":"6e474ae3-6265-43fb-c99c-706045eeff11"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== CPU Inference Measurement START ===\n","Loading top‐1 configs from '/content/drive/MyDrive/Brandon's FYP/Hyperparameter Optimization.xlsx' (sheet 'Top 3 Configurations')…\n","Found 3 top-1 configurations (one per model).\n","Building test DataLoader from CSV: 'data/test/_classes.csv' and image dir: 'data/test'…\n","Test set contains 1824 images, 4 classes.\n","\n","*** Evaluating model: ResNet50 ***\n","  Warm-up pass…\n","  Timing inference on 1824 images (batch_size=32)…\n","  ► ResNet50 CPU inference time: 529.87 s\n","\n","*** Evaluating model: EfficientNetB2 ***\n","  Warm-up pass…\n","  Timing inference on 1824 images (batch_size=32)…\n","  ► EfficientNetB2 CPU inference time: 270.27 s\n","\n","*** Evaluating model: MobileNetV3 ***\n","  Warm-up pass…\n","  Timing inference on 1824 images (batch_size=32)…\n","  ► MobileNetV3 CPU inference time: 79.76 s\n","\n","=== All models evaluated ===\n","            model  cpu_inference_time_s  num_images  val_accuracy\n","0        ResNet50            529.872412        1824      0.994027\n","1  EfficientNetB2            270.269776        1824      0.997285\n","2     MobileNetV3             79.764669        1824      0.992398\n","Exporting results to '/content/drive/MyDrive/Brandon's FYP/cpu_inference_time_full.csv'…\n","Saved CPU timing results successfully.\n","=== CPU Inference Measurement END ===\n","\n"]}]},{"cell_type":"markdown","source":["# Auto Disconnect After Training"],"metadata":{"id":"IrekXjsjUzO8"}},{"cell_type":"code","source":["import time\n","from google.colab import runtime\n","\n","def disconnect_after(minutes=5):\n","    print(f\"Will disconnect Colab in {minutes} minutes if still running...\")\n","    time.sleep(minutes * 60)\n","    print(\"Disconnecting now.\")\n","    runtime.unassign()\n","\n","if __name__ == '__main__':\n","    disconnect_after(minutes=5)"],"metadata":{"id":"2gYp43gIUxWG","executionInfo":{"status":"ok","timestamp":1749491793486,"user_tz":-480,"elapsed":300233,"user":{"displayName":"FYP","userId":"06650288711143266396"}},"colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"e2b62ed1-a082-446b-e7aa-a7f0b7866340"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Will disconnect Colab in 5 minutes if still running...\n","Disconnecting now.\n"]}]}]}